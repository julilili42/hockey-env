@article{papoudakis2019survey,
  title   = {A Survey and Critique of Multiagent Deep Reinforcement Learning},
  author  = {Papoudakis, Georgios and Christianos, Filippos and Sch{\"a}fer, Lukas and Albrecht, Stefano V.},
  journal = {Autonomous Agents and Multi-Agent Systems},
  year    = {2021},
  volume  = {35},
  number  = {2},
  pages   = {34},
  doi     = {10.1007/s10458-021-09506-5}
}

@article{lillicrap2015ddpg,
  title   = {Continuous Control with Deep Reinforcement Learning},
  author  = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal = {arXiv preprint arXiv:1509.02971},
  year    = {2015},
  url     = {https://arxiv.org/abs/1509.02971}
}


@inproceedings{eberhard2023pink,
  title     = {Pink Noise Is All You Need: Colored Exploration for Deep Reinforcement Learning},
  author    = {Eberhard, Onno and Hollenstein, Jakob and Pinneri, Carlo and Martius, Georg},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2023},
  url       = {https://openreview.net/forum?id=imxyoQIC5XT}
}

@InProceedings{fujimoto2018:TD3, title = {Addressing Function Approximation Error in Actor-Critic Methods}, author = {Fujimoto, Scott and van Hoof, Herke and Meger, David}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {1587--1596}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsm√§ssan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf}, url = {http://proceedings.mlr.press/v80/fujimoto18a.html}, abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.} }


@misc{gymnasium2023,
  author       = {{Farama Foundation}},
  title        = {Gymnasium: A Standard API for Reinforcement Learning Environments},
  year         = {2023},
  howpublished = {\url{https://github.com/Farama-Foundation/Gymnasium}},
  note         = {Accessed: 2026-02-15}
}

@inproceedings{bengio2009curriculum,
  title     = {Curriculum Learning},
  author    = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle = {Proceedings of the 26th International Conference on Machine Learning (ICML)},
  year      = {2009},
  pages     = {41--48},
  publisher = {ACM},
  doi       = {10.1145/1553374.1553380}
}

@inproceedings{schaul2016per,
  title={Prioritized Experience Replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{bansal2018,
  title     = {Emergent Complexity via Multi-Agent Competition},
  author    = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2018},
  url       = {https://openreview.net/forum?id=Sy0GnUxCb}
}


