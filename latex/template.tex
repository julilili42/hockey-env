\documentclass[11pt]{article}

\usepackage{times}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{graphics}
\usepackage{color}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}       % blackboard math symbols
\usepackage{amssymb}
\usepackage{graphicx}


\usepackage{lipsum}

\usepackage{geometry}
\geometry{left=2.8cm,right=2.8cm,top=2.6cm,bottom=2.6cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{hyperref}% should be the last package you include

\newcommand{\theteam}{}
\newcommand{\team}[1]{\def\theteam{#1}}


\fancyhead[L]{\theteam}
\fancyhead[R]{\thepage}
\cfoot{}

\setlength{\parindent}{0pt}

\team{abcdef: Julian Jurcevic}
\title{RL-Course 2025/26: Final Project Report}
\author{\theteam}

\begin{document}
\maketitle

\section{Introduction}\label{sec:intro}
Reinforcement Learning (RL) enables agents to learn decision-making strategies through interaction with an environment. At each time step, the agent observes the current state, selects an action, and receives a scalar reward. The objective is to maximize the expected cumulative reward over time. Continuous control tasks are particularly challenging due to high-dimensional action spaces and instability in value estimation.

In this project, we consider a competitive air hockey environment implemented in Gymnasium~\cite{gymnasium2023} and based on a physics simulation using Box2D~\cite{box2d2023}. Two agents compete in a zero-sum setting. Each player controls translation, rotation, and shooting through continuous actions. The observation space consists of positional and velocity information for both players and the puck. Episodes terminate when a goal is scored or when a time limit is reached.

The agent is trained by maximizing the reward signal provided by the environment. The reward includes goal outcomes and additional shaping components. Performance is evaluated separately using the win rate against predefined weak and strong opponents.

The goals of this work are i) implement the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for continuous control in the hockey environment.

\section{Method}
\label{sec:Method}

\subsection{Twin Delayed Deep Deterministic Policy Gradients}

We use Twin Delayed Deep Deterministic Policy Gradients (TD3) \cite{fujimoto2018:TD3}, an off-policy actor–critic algorithm for continuous control.

TD3 employs two critics, $Q_{\phi_1}$ and $Q_{\phi_2}$, and computes the Bellman target via clipped double Q-learning:
\[
y = r + \gamma (1-d)\min_{i=1,2} Q_{\phi_i'}(s', \tilde a').
\]
Taking the minimum reduces overestimation bias.

Target policy smoothing perturbs the target action:
\[
\tilde a' = \text{clip}\big(\pi_{\theta'}(s') + \epsilon, -c, c\big),
\quad \epsilon \sim \mathcal{N}(0,\sigma^2),
\]
which regularizes the critic by smoothing sharp value peaks.

Critics minimize the Bellman error
\[
\mathcal{L}(\phi_i) =
\mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}
\big[(Q_{\phi_i}(s,a)-y)^2\big].
\]

The actor is updated less frequently (delayed updates) and optimized to maximize $Q_{\phi_1}(s,\pi_\theta(s))$.  
Target networks are updated via Polyak averaging:
\[
\phi' \leftarrow \tau \phi + (1-\tau)\phi'.
\]


\subsection{Replay and Exploration}

Transitions $(s,a,r,s',d)$ are stored in a replay buffer to enable off-policy learning.

We consider both uniform and prioritized sampling, where transitions are drawn proportional to their TD error. Importance weighting is applied to reduce sampling bias.

Exploration is performed in action space:
\[
a = \pi_\theta(s) + \epsilon.
\]
We evaluate Gaussian, Ornstein–Uhlenbeck, and temporally correlated pink noise.  
An initial random phase populates the replay buffer.


\subsection{Noise Annealing}

To improve stability, exploration noise is annealed over training:
\[
\sigma_t = \max\left(\sigma_0 (1 - \tfrac{t}{T}), \sigma_{\min}\right).
\]
This enables broad exploration early and increased exploitation later.


\subsection{Self-Play and Opponent Scheduling}

Training is conducted in a two-player setting.  
We combine fixed scripted opponents (weak and strong) with self-play.

Policy snapshots are stored periodically in a finite pool and sampled as opponents during training.  
Opponent probabilities are scheduled over time: early training emphasizes weak opponents, while later stages increase strong and self-play opponents.

This curriculum improves robustness and reduces overfitting to fixed behaviors.


\subsection{Network Architecture}

Actor and critics are fully connected networks with two hidden layers of 256 units and $\tanh$ activations.  
The actor outputs actions in $[-1,1]$, and critics receive concatenated state–action pairs.



\section{Experimental Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/weak_combined.pdf}
    \caption{
    Training performance against the weak opponent.
    The blue curve shows the 100-episode moving average return with one standard deviation shading.
    Red markers denote evaluation win rate measured every 200 episodes.
    The dashed line indicates random performance.
    }
    \label{fig:weak_training}
\end{figure}


\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
